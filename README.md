# Machine_Learning_Practical

# CW1: Identifying and Investigating the Issue of Overfitting in a Deep Neural Network

The goal of this project was to examine overfitting within a deep neural network and to learn how to prevent this common issue. We examined hyper-parameter tuning as well as regularization techniques such as L1, L2, and Dropout to analyze the effectiveness in reducing overfitting. These experiments were ran with the EMNIST Balanced dataset.

# CW2: Analyzing and Improving the Performance of a Convolutional Neural Network with Vanishing Gradient Problem

After identifying the vanishing gradient problem within the convolutional neural network, batch normalization was implemented to improve the performance of the CNN. Various hyper-parameter experiments were ran on CIFAR100 dataset to continuously improve the accuracy of the model. A literature review of batch normalization as well as other methods used to solve the gradient problem were examined.
